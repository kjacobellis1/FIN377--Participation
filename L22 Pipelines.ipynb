{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example from last class/ASGN 06- std'zing variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "\n",
    "import pandas as pd\n",
    "url = 'https://github.com/LeDataSciFi/lectures-spr2020/blob/master/assignment_data/Fannie_Mae_Plus_Data.gzip?raw=true'\n",
    "fannie_mae = pd.read_csv(url,compression='gzip')\n",
    "\n",
    "#one line: standardize all numeric vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kathrynjaco08/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:172: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.74</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.32</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.69</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.53</td>\n",
       "      <td>9.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.00</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.78</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>134007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.81</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.52</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>135007.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>12.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>132396.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.81</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>134481.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.82</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>12.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24322.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.87</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>67366.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.94</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24322.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>6.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.81</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.76</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.54</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.72</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.65</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.78</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.24</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>135038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.59</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  mean  std   min   25%   50%   75%    max\n",
       "0   135038.0  -0.0  1.0 -1.74 -0.87  0.00  0.87   1.73\n",
       "1   135038.0  -0.0  1.0 -2.32 -0.77  0.01  0.69   4.47\n",
       "2   135038.0   0.0  1.0 -1.66 -0.74 -0.23  0.53   9.02\n",
       "3   135038.0   0.0  1.0 -3.00 -0.81  0.64  0.64   0.64\n",
       "4   135038.0  -0.0  1.0 -3.78 -0.57  0.28  0.57   1.54\n",
       "5   134007.0   0.0  1.0 -3.81 -0.56  0.24  0.52   4.05\n",
       "6   135007.0   0.0  1.0 -1.16 -1.16  0.81  0.81  12.61\n",
       "7   132396.0   0.0  1.0 -2.81 -0.72 -0.03  0.76   2.67\n",
       "8   134481.0   0.0  1.0 -7.14 -0.66  0.24  0.82   2.01\n",
       "9   135038.0   0.0  1.0 -0.14 -0.14 -0.14 -0.14  12.13\n",
       "10  135038.0   0.0  1.0 -1.78 -0.82  0.01  0.97   1.48\n",
       "11   24322.0  -0.0  1.0 -2.48 -0.81  0.17  0.87   3.67\n",
       "12   67366.0  -0.0  1.0 -6.94 -0.60  0.27  0.79   1.95\n",
       "13   24322.0  -0.0  1.0 -0.30 -0.30 -0.30 -0.30   6.99\n",
       "14  135038.0  -0.0  1.0 -1.42 -0.76 -0.28  0.81   2.32\n",
       "15  135038.0  -0.0  1.0 -1.76 -1.01  0.17  0.95   1.73\n",
       "16  135038.0   0.0  1.0 -4.54 -0.51  0.05  0.61   2.34\n",
       "17  135038.0  -0.0  1.0 -1.64 -0.96  0.16  0.72   2.64\n",
       "18  135038.0   0.0  1.0 -1.59 -0.92 -0.27  0.92   2.79\n",
       "19  135038.0   0.0  1.0 -1.65 -0.76 -0.47  0.90   2.42\n",
       "20  135038.0   0.0  1.0 -1.90 -0.63  0.06  0.78   2.29\n",
       "21  135038.0   0.0  1.0 -2.71 -0.38  0.10  0.62   2.81\n",
       "22  135038.0   0.0  1.0 -1.18 -0.99  0.02  0.86   1.88\n",
       "23  135038.0  -0.0  1.0 -2.24 -0.75 -0.12  0.86   2.18\n",
       "24  135038.0   0.0  1.0 -1.60 -0.89 -0.15  0.59   2.31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing \n",
    "fannie_array = preprocessing.scale(fannie_mae.select_dtypes('number'))\n",
    "\n",
    "pd.DataFrame(fannie_array).describe().round(2).T\n",
    "\n",
    "# remaining issues you should fix in assignment...\n",
    "# - only do this to true continuous variables\n",
    "# - only keep variables of interest\n",
    "# - impute missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cardinal Sin of data leakage\n",
    "**Having data in the training sample that you wouldnt have for real world predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples\n",
    "1. y is explicitly in X (yikes)\n",
    "2. y is a 2018 variable, but there is a 2019 variable in X\n",
    "3. subtle: y is loan default, but X contains employee ID and some employees are brought in to handle trouble-loans (if you include it, the firm can't use the model to deploy the trouble-loan specialists)\n",
    "4. if out-of-sample predicted stock movements have R2 above 10%... unlikely! (or: you'll be richer than Bezos soon)\n",
    "5. this code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import #a bunch of sklearn stuff\n",
    "X, y = #load data\n",
    "X = transform(X) # imputation, encode cat vars, standardize\n",
    "\n",
    "# or this:\n",
    "cross_validate(model,X,y)\n",
    "\n",
    "```\n",
    "\n",
    "**Q: What's the problem here?**\n",
    "\n",
    "**A: `transform(X)` used the whole dataset, so the X_training data was altered using info from X_test** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 sample\n",
    "1 training\n",
    "1 training \n",
    "\n",
    "2 test\n",
    "1 test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Data Leakage\n",
    "\n",
    "- Preventing 1-4: Be very familiar with the data and how it was collected and built\n",
    "- Preventing 5: Do your data prep _**within**_ CV folds and where the transformations are done using only info from the training \n",
    "\n",
    "```python\n",
    "\n",
    "# loop over folds \n",
    "for train_index, test_index in StratifiedKFold(n_splits=5).split(X,y):\n",
    "\n",
    "    # .split() yields the indices in train/test sets. use those to get \n",
    "    # the x/y vars for each separated out:\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] ",
    "\n",
    "From Donald Bowen to Everyone: (01:03 PM)\n",
    " ",
    "###################################################################\n",
    "    # NEW: do the data prep inside this fold, only using training data \n",
    "    ###################################################################\n",
    "\n",
    "    # e.g. figure out means/std in Xtrain so we can impute/std\n",
    "    prep_methods.fit(Xtrain)                 # \"fit\" the transform means \"estimate (like in training a model) what to do\"\n",
    "    Xtrain = prep_methods.transform(Xtrain)  # apply those to Xtrain to impute and std\n",
    "    \n",
    "    # fit/estimate, predict OOS, evaluate and store\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    ###################################################################\n",
    "    # NEW: transform the test data the same... \n",
    "    ###################################################################\n",
    "    \n",
    "    X_test = prep_methods.transform(X_test)  # apply TEST data the FIT from the TRAIN data \n",
    "    \n",
    "    y_predict = model.predict(X_test)\n",
    "    accuracy.append(   accuracy_score(y_test, y_predict)      )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first pipeline\n",
    "\n",
    "Pipe: a sequence of steps, as long as each step has a fit, and a transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validate(model,X,y)\n",
    "model in sklearn --> .fit, and .predict() / transform #predict is sort of like a transformation in an abstract sense\n",
    "prep_method #fictional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00620794, 0.00116682, 0.00102401, 0.00115275, 0.00100994]),\n",
       " 'score_time': array([0.00078297, 0.00034523, 0.00032687, 0.00033402, 0.00032711]),\n",
       " 'test_score': array([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1.        ])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "\n",
    "iris = load_iris() # data\n",
    "\n",
    "# set up the pipeline, which will, given a set of observations \n",
    "# 1. fit and apply these steps to the training fold\n",
    "# 2. in the testing fold, apply the transform and model to predict (no estimation)\n",
    "\n",
    "classifier_pipeline = make_pipeline(\n",
    "                                    preprocessing.StandardScaler(),  # clean the data\n",
    "                                    svm.SVC(C=1)                     # model\n",
    "                                    )\n",
    "\n",
    "cross_validate(classifier_pipeline, iris.data, iris.target, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00467682, 0.001858  , 0.00182033, 0.00150299, 0.00233507]),\n",
       " 'score_time': array([0.00625205, 0.00345302, 0.00334573, 0.00212717, 0.00292802]),\n",
       " 'test_score': array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1.        ])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 1: try this with a Nearest Neighbors Classifier \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_pipeline = make_pipeline(\n",
    "                                    preprocessing.StandardScaler(),  # clean the data\n",
    "                                    KNeighborsClassifier()                     # model\n",
    "                                    )\n",
    "\n",
    "cross_validate(classifier_pipeline, iris.data, iris.target, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00640702, 0.00362015, 0.00725293, 0.00743818, 0.00321794]),\n",
       " 'score_time': array([0.00308275, 0.00278878, 0.00463605, 0.00507283, 0.00239468]),\n",
       " 'test_score': array([0.9       , 0.96666667, 0.9       , 0.96666667, 1.        ])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 2: load this altered dataset and add a step to impute the missing values with the column mean\n",
    "\n",
    "iris2=load_iris()\n",
    "X2= pd.DataFrame(iris2.data)\n",
    "X2.columns = [1,2,3,4]\n",
    "X2[2] = X2[2].sample(frac=0.5,random_state=14)\n",
    "X2[2].describe()\n",
    "iris2.data = X2\n",
    "\n",
    "# print the scores using IRIS2.data (not iris.data)\n",
    "# this produces an error because of the missing values!\n",
    "# cross_validate(knn_pipe, iris2.data, iris.target, cv=5)\n",
    "\n",
    "# so add an imputation step to the pipeline! (5 min, use lecture page!)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "knn_pipe2 = make_pipeline(\n",
    "                        SimpleImputer(strategy='mean'), #fill the missing values\n",
    "                        preprocessing.StandardScaler(),  # clean the data\n",
    "                        KNeighborsClassifier()          # model\n",
    "                        )\n",
    "\n",
    "\n",
    "cross_validate(knn_pipe2, iris2.data, iris.target, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing a model- here, `KKN`, with `GridSearchCV`\n",
    "\n",
    "Let's optimize the model from the last answer... `GridSearchCV` lets you tweak any perameters from any function in the pipeline\n",
    "\n",
    "Tips:\n",
    "    - If any perameteres in your grid are optimal at the boundaries, add more points until optimum is inferior\n",
    "    - After you optimize the model, save it as a model object to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('simpleimputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('kneighborsclassifier',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=None, n_neighbors=5, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kathrynjaco08/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/Kathrynjaco08/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#grid search will let you specify all the perameters of the mdoel\n",
    "#you want to tweak, and the values you want to try\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set up parameter grid to try\n",
    "# the parameter grid is a dictionary where key:value pairs are built like:\n",
    "#       stepName<two underlines>paramName : [list of settings to try]\n",
    "param_grid = {'kneighborsclassifier__n_neighbors':[1,5,6,7,8,9,10]}\n",
    "\n",
    "# like a normal estimator, this has not yet been applied to any data\n",
    "grid = GridSearchCV(knn_pipe2, param_grid=param_grid)\n",
    "grid.fit(iris2.data, iris.target)\n",
    "grid.best_params_\n",
    "\n",
    "# now save that pipeline as a model object!\n",
    "optimal_knn_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kathrynjaco08/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/Kathrynjaco08/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'kneighborsclassifier__n_neighbors': 9,\n",
       " 'standardscaler__with_mean': 'True',\n",
       " 'standardscaler__with_std': 'True'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#question 3: add to the param grid to check if we should change these two params:\n",
    "#            StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set up parameter grid to try\n",
    "# the parameter grid is a dictionary where key:value pairs are built like:\n",
    "#       stepName<two underlines>paramName : [list of settings to try]\n",
    "param_grid = {'kneighborsclassifier__n_neighbors':[1,5,6,7,8,9,10],\n",
    "             'standardscaler__with_mean':['True','False'],\n",
    "             'standardscaler__with_std':['True','False']}\n",
    "\n",
    "# like a normal estimator, this has not yet been applied to any data\n",
    "grid = GridSearchCV(knn_pipe2, param_grid=param_grid)\n",
    "grid.fit(iris2.data, iris.target)\n",
    "grid.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00684071, 0.00513458, 0.00877579, 0.00487566, 0.00367864,\n",
       "        0.00390704, 0.00332054, 0.00471258, 0.00338968, 0.00366871,\n",
       "        0.00467237, 0.00397142, 0.00372084, 0.00421   , 0.00351389,\n",
       "        0.00345254, 0.0032088 , 0.00453496, 0.00420078, 0.00446335,\n",
       "        0.00513442, 0.00386103, 0.00551287, 0.00540789, 0.00541401,\n",
       "        0.00641306, 0.00536577, 0.00478268]),\n",
       " 'std_fit_time': array([2.43707652e-03, 1.26350251e-03, 3.60079946e-03, 1.04308567e-03,\n",
       "        2.92842100e-04, 6.68243579e-04, 3.17222428e-04, 1.89248049e-03,\n",
       "        1.27768089e-04, 2.85923182e-04, 9.65317194e-04, 1.93247569e-04,\n",
       "        9.94107955e-05, 5.93172807e-04, 3.16526523e-04, 2.23807454e-04,\n",
       "        1.33014540e-04, 8.87237917e-04, 2.61693134e-04, 4.30525878e-04,\n",
       "        1.87675942e-03, 5.50857767e-04, 9.12498481e-04, 7.51196985e-04,\n",
       "        1.20650765e-03, 2.11197061e-03, 1.50454620e-03, 1.25402195e-03]),\n",
       " 'mean_score_time': array([0.00467436, 0.00671371, 0.0051233 , 0.00386906, 0.00320609,\n",
       "        0.00306662, 0.00438062, 0.00561945, 0.00289408, 0.00316429,\n",
       "        0.00354664, 0.00335399, 0.00311391, 0.00374158, 0.00321039,\n",
       "        0.00389878, 0.00413164, 0.00350014, 0.00342798, 0.00302434,\n",
       "        0.00400559, 0.00420944, 0.00517821, 0.00506608, 0.00491802,\n",
       "        0.0053273 , 0.0048089 , 0.00484363]),\n",
       " 'std_score_time': array([0.00102572, 0.00123323, 0.00065845, 0.00097553, 0.00071798,\n",
       "        0.00019577, 0.00087308, 0.00172425, 0.00020617, 0.00028198,\n",
       "        0.00051624, 0.00043912, 0.00045376, 0.00034832, 0.00025687,\n",
       "        0.00078147, 0.00031772, 0.00042724, 0.00013285, 0.00018011,\n",
       "        0.00035254, 0.00094043, 0.00151359, 0.00096445, 0.00159464,\n",
       "        0.00184192, 0.00137782, 0.00099957]),\n",
       " 'param_kneighborsclassifier__n_neighbors': masked_array(data=[1, 1, 1, 1, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8,\n",
       "                    8, 8, 9, 9, 9, 9, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_standardscaler__with_mean': masked_array(data=['True', 'True', 'False', 'False', 'True', 'True',\n",
       "                    'False', 'False', 'True', 'True', 'False', 'False',\n",
       "                    'True', 'True', 'False', 'False', 'True', 'True',\n",
       "                    'False', 'False', 'True', 'True', 'False', 'False',\n",
       "                    'True', 'True', 'False', 'False'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_standardscaler__with_std': masked_array(data=['True', 'False', 'True', 'False', 'True', 'False',\n",
       "                    'True', 'False', 'True', 'False', 'True', 'False',\n",
       "                    'True', 'False', 'True', 'False', 'True', 'False',\n",
       "                    'True', 'False', 'True', 'False', 'True', 'False',\n",
       "                    'True', 'False', 'True', 'False'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'kneighborsclassifier__n_neighbors': 1,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 1,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 1,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 1,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 5,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 5,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 5,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 5,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 6,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 6,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 6,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 6,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 7,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 7,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 7,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 7,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 8,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 8,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 8,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 8,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 9,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 9,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 9,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 9,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 10,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 10,\n",
       "   'standardscaler__with_mean': 'True',\n",
       "   'standardscaler__with_std': 'False'},\n",
       "  {'kneighborsclassifier__n_neighbors': 10,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'True'},\n",
       "  {'kneighborsclassifier__n_neighbors': 10,\n",
       "   'standardscaler__with_mean': 'False',\n",
       "   'standardscaler__with_std': 'False'}],\n",
       " 'split0_test_score': array([0.94117647, 0.94117647, 0.94117647, 0.94117647, 0.90196078,\n",
       "        0.90196078, 0.90196078, 0.90196078, 0.90196078, 0.90196078,\n",
       "        0.90196078, 0.90196078, 0.90196078, 0.90196078, 0.90196078,\n",
       "        0.90196078, 0.90196078, 0.90196078, 0.90196078, 0.90196078,\n",
       "        0.92156863, 0.92156863, 0.92156863, 0.92156863, 0.92156863,\n",
       "        0.92156863, 0.92156863, 0.92156863]),\n",
       " 'split1_test_score': array([0.90196078, 0.90196078, 0.90196078, 0.90196078, 0.88235294,\n",
       "        0.88235294, 0.88235294, 0.88235294, 0.92156863, 0.92156863,\n",
       "        0.92156863, 0.92156863, 0.92156863, 0.92156863, 0.92156863,\n",
       "        0.92156863, 0.94117647, 0.94117647, 0.94117647, 0.94117647,\n",
       "        0.94117647, 0.94117647, 0.94117647, 0.94117647, 0.94117647,\n",
       "        0.94117647, 0.94117647, 0.94117647]),\n",
       " 'split2_test_score': array([0.9375    , 0.9375    , 0.9375    , 0.9375    , 0.97916667,\n",
       "        0.97916667, 0.97916667, 0.97916667, 0.97916667, 0.97916667,\n",
       "        0.97916667, 0.97916667, 0.97916667, 0.97916667, 0.97916667,\n",
       "        0.97916667, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
       "        0.95833333, 0.95833333, 0.95833333, 0.95833333, 0.95833333,\n",
       "        0.95833333, 0.95833333, 0.95833333]),\n",
       " 'mean_test_score': array([0.92666667, 0.92666667, 0.92666667, 0.92666667, 0.92      ,\n",
       "        0.92      , 0.92      , 0.92      , 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.94      , 0.94      , 0.94      , 0.94      , 0.94      ,\n",
       "        0.94      , 0.94      , 0.94      ]),\n",
       " 'std_test_score': array([0.01779513, 0.01779513, 0.01779513, 0.01779513, 0.04138532,\n",
       "        0.04138532, 0.04138532, 0.04138532, 0.03246416, 0.03246416,\n",
       "        0.03246416, 0.03246416, 0.03246416, 0.03246416, 0.03246416,\n",
       "        0.03246416, 0.02357023, 0.02357023, 0.02357023, 0.02357023,\n",
       "        0.0149509 , 0.0149509 , 0.0149509 , 0.0149509 , 0.0149509 ,\n",
       "        0.0149509 , 0.0149509 , 0.0149509 ]),\n",
       " 'rank_test_score': array([21, 21, 21, 21, 25, 25, 25, 25,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "         9,  9,  9,  1,  1,  1,  1,  1,  1,  1,  1], dtype=int32)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you can see the WHOLE set of attempts by GridSearch using\n",
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do some post-optimization diagnostics\n",
    "1. scoring on `kfold`\n",
    "2. graphical if y is continuous\n",
    "3. categorical accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        27\n",
      "  versicolor       0.93      0.96      0.94        26\n",
      "   virginica       0.95      0.91      0.93        22\n",
      "\n",
      "    accuracy                           0.96        75\n",
      "   macro avg       0.96      0.96      0.96        75\n",
      "weighted avg       0.96      0.96      0.96        75\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/Users/Kathrynjaco08/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-68c47b3188d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/Users/Kathrynjaco08/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/__init__.py)"
     ]
    }
   ],
   "source": [
    "# print k-fold scoring (like before)\n",
    "cross_validate(optimal_knn_model, iris2.data, iris.target, cv=5)\n",
    "\n",
    "###########################################################\n",
    "# use classification_report to see which types of Y values \n",
    "# your prediction performs better/worse on\n",
    "###########################################################\n",
    "\n",
    "# to use class_report, we need some predicted y values, so\n",
    "# make a fold and generate predicted values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(iris2.data, iris.target, random_state=9,train_size=.5)\n",
    "y_pred = optimal_knn_model.fit(Xtrain, ytrain).predict(Xtest)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest,\n",
    "                            y_pred,\n",
    "                            target_names=iris.target_names))\n",
    "#################################################################\n",
    "# use confusion_matrix see exactly model gets predictions wrong\n",
    "#################################################################\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_confusion_matrix(optimal_knn_model, Xtest, ytest,   # model and test data\n",
    "                      display_labels=iris.target_names,  # labels\n",
    "                      cmap=plt.cm.Blues,                 # colors\n",
    "                      normalize=None)                    # turns on/off fractions (within row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "- We've now seen more post model diagnostics \n",
    "- We can specify the models in `make_pipeline` alongside data cleaning/preprocessing steps that improve model performance without introducing data leakage. \n",
    "- There are many imputation, and scaling methods available in `sklearn`, and which one you use depends on the use-case. (Read about and try several!)\n",
    "- Your pipeline for the assignment will be more complicated if you want to include categorical vars\n",
    "- You can optimize all of the parameters throughout your pipeline using `GridSearchCV`\n",
    "    - `GridSearchCV` also allows you to specify how you create folds\n",
    "    - Which leads us to...\n",
    "\n",
    "**LAST BIG POINT:** \n",
    "- Must of your projects involve an important time series dimension. (Ex: predicting stock returns) \n",
    "- In these cases, `KFold` and `StratifiedKFold` won't work (you can't have 1985 in the test sample)\n",
    "- See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
